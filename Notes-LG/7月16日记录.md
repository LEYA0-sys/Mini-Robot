# UV工具的安装与使用

## 什么是uv工具

`uv` 是一个用于 Python 包管理的现代工具，它旨在替代 `pip` 和 `virtualenv`，提供更快、更高效的包安装与虚拟环境管理体验。`uv` 是由 Astral 团队开发的，并逐渐成为 Python 社区中的一个热门替代工具。

简而言之，`uv` 是一个 **超快的 Python 包管理器**，它集成了：

- 包安装（替代 `pip`）
- 虚拟环境管理（替代 `virtualenv`）
- 锁定文件（兼容 `pip-tools` 和 `Poetry`）

## uv的优势

- 🚀 **极快的安装速度**：基于 Rust 实现，速度比 pip 快上几十倍
- 📦 **自动创建虚拟环境**：类似 Poetry，无需手动激活
- 🔐 **锁定依赖**：生成 `uv.lock` 文件，确保可重复构建
- 🐍 **兼容性强**：兼容 `pyproject.toml`，无缝对接现有项目
- 📁 **缓存机制优秀**：充分利用缓存，加快安装流程

## uv工具快速使用

以下是快速上手 uv 的步骤：

### 安装 uv

`Linux`操作系统可以使用以下命令安装

```
curl -Ls https://astral.sh/uv/install.sh | bash
```

如果电脑中安装过`Python`，可以直接使用`pip`命令安装：

```
pip install uv
```

### 初始化项目

在桌面创建一个空文件夹`uv_test`

![image-20250417180502853](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417180502853.png)

打开文件夹，右键文件夹空白处，点击在终端打开

![image-20250417180525916](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417180525916.png)

输入`uv init`即可初始化`uv`工程。

```
uv init
```

![image-20250417180621322](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417180621322.png)

> `uv`工程默认会生成4个文件：
>
> `.python-version`：记录当前工程的`Python`版本。
>
> `main.py`：主脚本。
>
> `pyproject.toml`：记录当前`uv`工程的依赖情况。
>
> `README.md`：工程的说明文件。

### 生成虚拟环境

使用以下命令可以为项目初始化一个`Python`虚拟环境，并且可以指定`Python`解释器版本。

```
uv venv --python 3.10
```

![image-20250417181245476](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181245476.png)

> 你可以通过以下命令直接创建工程并指定`python`解释器版本。
>
> 复制
>
> ```
> uv init uv_test -p 3.10
> ```

### 激活虚拟环境

输入以下命令即可激活虚拟环境

```
.\.venv\Scripts\activate
```

![image-20250417181358365](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181358365.png)

### 下载第三方库

```
uv add pandas opencv-python
```

![image-20250417181502610](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181502610.png)

输入命令`uv tree`，可以查看当前虚拟环境中的依赖树

```
uv tree
```

![image-20250417181541837](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181541837.png)

可以看到前面安装的`pandas`和`opencv`都安装成功了。

## 卸载第三方库

例如，卸载`pandas`库，使用以下命令



```
uv remove pandas
```

![image-20250417181657976](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181657976.png)

再次查看依赖树

```
uv tree
```

![image-20250417181719673](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181719673.png)

可以看到`pandas`库已经被卸载。

### 执行命令

初始化`uv`工程时，它会自动创建一个主脚本`main.py`，使用以下命令执行`Python`脚本。

```
uv run main.py
```

![image-20250417182312761](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417182312761.png)

## uv的其他基本命令

### uv pip

```
uv`工具中也有`pip`功能可以调用，通过`uv pip`进行调用，例如下载`pandas`库，也可以使用`pip
```

```
uv pip install pandas
```

![image-20250417181900514](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181900514.png)

通过`pip`下载的内容，需要使用以下命令查看依赖列表。

```
uv pip list
```

![image-20250417181958953](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417181958953.png)

> 如果你不适应`uv`的命令，你也完全可以通过`uv pip`的方式去调用`pip`工具的相关命令，它比原生的`pip`命令更为高效。

### uv sync

每个`uv`工程中都有一个`pyproject.toml`文件，用于记录该工程的依赖环境。每次使用`uv`命令去下载或者修改依赖都会造成`pyproject.toml`文件的变化。

在使用别人的`uv`工程时，可以通过`uv sync`命令一键克隆其环境依赖（包含`Python`版本）。

> 注意：使用`uv pip`安装的依赖，不会记录在`pyproject.toml`文件中。

### uv python

`uv`可以提前缓存多个版本的`python`解释器，通过以下命令进行查看并安装。

```
# 显示当前已经安装的和可供安装的Python版本
uv python list
```

![image-20250417182813189](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417182813189.png)

已经安装好的，在输出结果中会呈现蓝色。

```
# 你可以提前缓存python指定版本，这样后续在其他项目中安装Python环境时，不需要下载。
uv python install 3.10 3.11 3.12
```

![image-20250417183002601](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417183002601.png)

```
# 卸载缓存的指定Python版本
uv python uninstall 3.12
```

![image-20250417183038541](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417183038541.png)

```
# 切换当前工程的Python版本
uv python pin 3.11
uv sync
```

![image-20250417184402723](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250417184402723.png)

> 此命令只会先修改`.python-version`文件内容，如果要更新`Python`解释器文件，则需要执行`uv sync`，因为修改了`Python`版本，对应的依赖也需要跟也`Python`版本去重新更新。

# 从0开始实现MCP-Server

## MCP Server概念

`MCP Server` 是一个中间层服务器，它主要负责处理和管理 AI 模型的上下文信息，确保模型能够高效且准确地理解和响应用户请求。它作为应用程序和`AI`模型之间的桥梁，优化了信息的传递和处理过程。

根据MCP协议定义，Server可以提供三种类型的标准能力，**Resources、Tools、Prompts**，每个Server可同时提供者三种类型能力或其中一种。

- **Resources：**资源，类似于文件数据读取，可以是文件资源或是API响应返回的内容。
- **Tools：**工具，第三方服务、功能函数，通过此可控制LLM可调用哪些函数。
- **Prompts：**提示词，为用户预先定义好的完成特定任务的模板。

## MCP通信方式

`MCP（Model Context Protocol）`是一种为了统一大规模模型和工具间通信而设计的协议，它定义了消息格式和通信方式。MCP 协议支持多种传输机制，其中包括 `stdio`、`Server-Sent Events（SSE）` 和 `Streamable HTTP`。

## Stdio 传输（Standard Input/Output）

`stdio` 传输方式是最简单的通信方式，通常在本地工具之间进行消息传递时使用。它利用标准输入输出（`stdin/stdout`）作为数据传输通道，适用于本地进程间的交互。

- **工作方式**：客户端和服务器通过标准输入输出流（`stdin/stdout`）进行通信。客户端向服务器发送命令和数据，服务器执行并通过标准输出返回结果。
- **应用场景**：适用于本地开发、命令行工具、调试环境，或者模型和工具服务在同一进程内运行的情况。

## Server-Sent Events（SSE）

`SSE` 是基于 `HTTP` 协议的流式传输机制，它允许服务器通过 `HTTP` 单向推送事件到客户端。`SSE` 适用于客户端需要接收服务器推送的场景，通常用于实时数据更新。

- **工作方式**：客户端通过 `HTTP GET` 请求建立与服务器的连接，服务器以流式方式持续向客户端发送数据，客户端通过解析流数据来获取实时信息。
- **应用场景**：适用于需要服务器主动推送数据的场景，如实时聊天、天气预报、新闻更新等。

## Streamable HTTP

`Streamable HTTP` 是 `MCP` 协议中新引入的一种传输方式，它基于 `HTTP` 协议支持双向流式传输。与传统的 `HTTP` 请求响应模型不同，`Streamable HTTP` 允许服务器在一个长连接中实时向客户端推送数据，并且可以支持多个请求和响应的流式传输。

不过需要注意的是，`MCP`只提供了`Streamable HTTP`协议层的支持，也就是规范了`MCP`客户端在使用`Streamable HTTP`通信时的通信规则，而并没有提供相关的`SDK`客户端。开发者在开发`Streamable HTTP`机制下的客户端和服务器时，可以使用比如`Python httpx`库进行开发。

- **工作方式**：客户端通过 `HTTP``POST` 向服务器发送请求，并可以接收流式响应（如 `JSON-RPC` 响应或 `SSE` 流）。当请求数据较多或需要多次交互时，服务器可以通过长连接和分批推送的方式进行数据传输。
- **应用场景**：适用于需要支持高并发、低延迟通信的分布式系统，尤其是跨服务或跨网络的应用。适合高并发的场景，如实时流媒体、在线游戏、金融交易系统等。

## MCP Server实现流程

在本教程中将带领大家一起实现一个类似于`MCP`官网的天气查询`MCP Server`，但是与官网的`MCP`示例不同的是，官网的天气查询仅仅支持美国的州市，无法查询中国城市的天气情况。所以，在本教程中，使用的是`openweather`的免费接口，实现全世界各地的一个通用天气查询`MCP`服务。

## 业务功能实现

进入`OpenWeather`官网（https://openweathermap.org/），然后使用自己的信息注册一个账号。

![image-20250418145737828](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418145737828.png)

接着我们需要申请一个`API Keys`，用于后期接口校验。点击`My APIKeys`。

![image-20250418145826431](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418145826431.png)

默认的情况下，会自动给你生成一个`API Keys`，你可以直接使用默认生成的`API Keys`，或者自己重新创建一个`API Keys`。

![image-20250418150008898](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150008898.png)

虽然两者都可以，但是建议大家直接使用默认的`API Keys`即可，因为创建新的`API Keys`后，需要等5分钟左右才能生效。默认的`API Keys`只需要3分钟左右即可使用。

复制自己的`API Keys`后，点击菜单栏上的`API`即可开始选择自己所需要的服务。

需要注意的是，`OpenWeather`有很多关于天气的服务，但是并不是所有服务都是免费的，你需要根据他的描述，选择自己所需要的服务即可，在这里我们直接选择`Current Weather Data`接口，该接口是免费的。点击其对应的`API doc`。

![image-20250418150312577](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150312577.png)

该接口也有很多种请求方式，我们选择两种。

1. 通过经纬度，请求对应经纬度的当前天气情况。

![image-20250418150456587](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150456587-1752657820345-47.png)

1. 通过城市名称，查询对应城市名称的当前天气情况。

![image-20250418150555253](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150555253.png)

无论是哪种请求方式，`API Key`都是必填参数。

我们有了自己的`API Key`后，可以直接通过浏览器请求的方式，验证当前接口是否可用。直接在浏览器中输入`url`即可。

比如，我通过指定城市名称为wuhan，查询武汉对应的天气情况。

```plain
https://api.openweathermap.org/data/2.5/weather?q=wuhan&appid={API key}
```

注意：{API key}需要替换为你自己的。(ligang:f543ca6fa6c3e75e8ac30cc2c89ca753)

有以下返回，说明接口是可用的。

![image-20250418150851828](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150851828.png)

如果返回以下内容，说明`API Key`未生效，或者`API Key`错误。如果是检查了`API Key`确定没有填写错误，那么请等待几分钟后重试。

![image-20250418150942804](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418150942804.png)

到这里，我们已经支持如何通过经纬度和地名获取天气了，接下来要做的就是将该服务封装未`MCP Server`。

## MCP Server功能编写

在这里，我们首先测试`stdio`通信方式，采用本地开启一个`MCP Server`的方式实现。

首先，使用`uv`工具，创建项目并安装相关依赖。这里我将项目放到`D`盘的根目录，在`D`盘下打开命令提示符。

```plain
uv init weather_mcp_server -p 3.10
```

![image-20250418152617215](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418152617215.png)

接着进入`uv`工程。

然后输入以下命令，创建虚拟环境。

![image-20250418152838376](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418152838376.png)

激活虚拟环境

![image-20250418152913928](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418152913928.png)

由于我电脑中默认会激活一个`conda`的`base`虚拟环境，所以再激活`uv`工程的虚拟环境后，在路径前面出现了两个`()`，故我还需要执行以下命令，退出`conda`的`base`虚拟环境。

安装依赖

![image-20250418153323532](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418153323532.png)

依赖准备完成，接下来开始代码编写部分内容。

`weather.py`代码如下所示：

注：代码中的`{API KEY}`部分，请替换为自己的`API Key`。

```python
from typing import Any
import httpx
from mcp.server.fastmcp import FastMCP


mcp = FastMCP("weather")


NWS_API_BASE = "https://api.openweathermap.org/data/2.5/weather"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"


def kelvin_to_celsius(kelvin: float) -> float:
    return kelvin - 273.15

async def get_weather_from_cityname(cityname: str) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "q": cityname,
        "appid": "{API KEY}"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None
        

async def get_weather_from_latitude_longitude(latitude: float, longitude: float) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "lat": latitude,
        "lon": longitude,
        "appid": "{API KEY}"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None

def format_alert(feature: dict) -> str:
    """将接口返回的天气信息进行格式化文本输出"""
    if feature["cod"] == 404:
        return "参数异常，请确认城市名称是否正确。"
    elif feature["cod"] == 401:
        return "API key 异常，请确认API key是否正确。"
    elif feature["cod"] == 200:
        return f"""
        City: {feature.get('name', 'Unknown')}
        Weather: {feature.get('weather', [{}])[0].get('description', 'Unknown')}
        Temperature: {kelvin_to_celsius(feature.get('main', {}).get('temp', 0)):.2f}°C
        Humidity: {feature.get('main', {}).get('humidity', 0)}%
        Wind Speed: {feature.get('wind', {}).get('speed', 0):.2f} m/s
        """
    else:
        return "未知错误，请稍后再试。"

@mcp.tool()
async def get_weather_from_cityname_tool(city: str) -> str:
    """Get weather information for a city.

    Args:
        city: City name (e.g., "wuhan"). For Chinese cities, please use pinyin
    """
    data = await get_weather_from_cityname(city)
    return format_alert(data)

@mcp.tool()
async def get_weather_from_latitude_longitude_tool(latitude: float, longitude: float) -> str:
    """Get weather information for a location.

    Args:
        latitude: Latitude of the location
        longitude: Longitude of the location
    """
    data = await get_weather_from_latitude_longitude(latitude, longitude)
    return format_alert(data)

if __name__ == "__main__":
    
    mcp.run(transport='stdio')
```

在该代码中，我们一共定义了两个`Tool`：

1. `get_weather_from_cityname_tool`：通过城市名称获取天气情况。
2. `get_weather_from_latitude_longitude_tool`：通过经纬度获取天气情况。

注意，由于`MCP`协议需要使用到`@mcp.tool`标记工具函数，所以使用`@mcp.tool`标记的工具函数，对应的注释**务必写清除**，后续大模型能够识别这些工具、工具如何使用以及工具功能，全都是通过这些注释进行解读的。所以一个好的`MCP Server`，其对应的`Tool`描述也必须要非常的清除。

![image-20250418152041716](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418152041716.png)

## MCP Server测试

我们编辑好代码后，可以直接在`cursor`中测试该`MCP Server`是否可以正常提供功能和被大模型调用。

```json
{
  "mcpServers": {
    "weather": {
          "command": "uv",
          "args": [
              "--directory",
              "D:\\weather_mcp_server",  
              "run",
              "weather.py"
          ]
      }
  }
}
```

设置完成后，可以发现`Cursor`可以成功加载我们自己写的`weather MCP`。

![image-20250418153509409](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418153509409.png)

接下来，在对话中测试，看看是否可以调用到天气查询服务。

![image-20250418153813844](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418153813844-1752679664496-3.png)

可以看到，我们自己编写的`MCP Server`可以成功被`Corsor`调用。

## MCP Server发布

前面我们演示的是`stdio`方式的通信协议，该方式本质上就是在本地允许了一个服务，然后通过`MCP Client`去调用本地的服务实现的。

这种方式的缺点是，无法将自己的`MCP Server`，在不把源代码给别人的情况下，共享给其他人使用。这种方式在企业中肯定是不能够被允许的，源代码是企业的命脉。此时，`SSE`通信方式就派上用场了，可以使用`SSE`的通信方式将自己的`MCP Server`部署在服务器，然后其他所有的`MCP Client`要调用`MCP Server`对应的服务时，就无需在本地去执行`MCP Server`服务了。

如果需要其他所有人都可以访问当你的`MCP Server`，就需要将自己的`MCP Server`配置到公网服务器。

在公网中配置环境的方式与前面的流程一致，需要按照以下方式修改代码即可。

将`weather_sse.py`代码修改为以下内容，配置为`sse`通信协议。

```plain
from typing import Any
import httpx
from mcp.server.fastmcp import FastMCP



mcp = FastMCP(
    name="weather",
    host="0.0.0.0",
    port=8000,
    description="通过城市名称（拼音）或经纬度获取天气信息",
    sse_path="/sse"
)


NWS_API_BASE = "https://api.openweathermap.org/data/2.5/weather"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"


def kelvin_to_celsius(kelvin: float) -> float:
    return kelvin - 273.15

async def get_weather_from_cityname(cityname: str) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "q": cityname,
        "appid": "24ecadbe4bb3d55cb1f06ea48a41ac51"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None
        

async def get_weather_from_latitude_longitude(latitude: float, longitude: float) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "lat": latitude,
        "lon": longitude,
        "appid": "24ecadbe4bb3d55cb1f06ea48a41ac51"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None

def format_alert(feature: dict) -> str:
    """将接口返回的天气信息进行格式化文本输出"""
    if feature["cod"] == 404:
        return "参数异常，请确认城市名称是否正确。"
    elif feature["cod"] == 401:
        return "API key 异常，请确认API key是否正确。"
    elif feature["cod"] == 200:
        return f"""
        City: {feature.get('name', 'Unknown')}
        Weather: {feature.get('weather', [{}])[0].get('description', 'Unknown')}
        Temperature: {kelvin_to_celsius(feature.get('main', {}).get('temp', 0)):.2f}°C
        Humidity: {feature.get('main', {}).get('humidity', 0)}%
        Wind Speed: {feature.get('wind', {}).get('speed', 0):.2f} m/s
        """
    else:
        return "未知错误，请稍后再试。"

@mcp.tool()
async def get_weather_from_cityname_tool(city: str) -> str:
    """Get weather information for a city.

    Args:
        city: City name (e.g., "wuhan"). For Chinese cities, please use pinyin
    """
    data = await get_weather_from_cityname(city)
    return format_alert(data)

@mcp.tool()
async def get_weather_from_latitude_longitude_tool(latitude: float, longitude: float) -> str:
    """Get weather information for a location.

    Args:
        latitude: Latitude of the location
        longitude: Longitude of the location
    """
    data = await get_weather_from_latitude_longitude(latitude, longitude)
    return format_alert(data)

if __name__ == "__main__":
    
    
    print("Starting server...")
    mcp.run(transport='sse')
```

在公网中执行该脚本，开启`MCP SSE Server`。执行完成后，可以看到如下图所示的打印结果。

![image-20250418161405895](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418161405895.png)

此时，再次使用`Cursor`测试`SSE MCP Server`服务是否可以正常调用。

在`Cursor`的`MCP`配置中，修改配置文件如下所示：

```plain
{
  "mcpServers": {
    "weather": {
      "url": "http://{你的公网IP}:8000/sse"
    }
  }
}
```

配置完成后，最好重启`Cursor`一次，因为前面我们加载过相同名字的`MCP`服务。

重启完成后，可以看到`Cursor`也是可以正常识别到我们的`MCP Server`。

![image-20250418161952944](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418161952944.png)

测试功能是否可以正常调用。

![image-20250418162419763](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418162419763.png)

功能也可以正常调用。

![image-20250418162834589](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250418162834589-1752666365613-73.png)

服务器也有请求响应。

# 从0开始实现MCP-Client

## 什么是MCP-Client？

`MCP-Client`是`Model Context Protocol`（模型上下文协议）架构中的一个重要组件，用于连接`AI`模型（如`Claude`、`GPT`等大型语言模型）与外部数据源、工具和服务的桥梁。

`MCP（Model Context Protocol）`是由`Anthropic`公司在2024年底首次提出并开源的一种开放标准协议，旨在解决大语言模型（`LLM`）与外部世界的连接问题。这一协议的核心价值在于打破了`AI`模型的”信息孤岛”限制，使模型能够以标准化的方式访问和处理实时数据，显著扩展了大模型的应用场景。

在`MCP`架构中，有三个关键组件：

1. MCP服务器（`Server`）：轻量级服务程序，负责对接具体数据源或工具（如数据库、`API`等），并按照MCP规范提供标准化的功能接口。每个`MCP`服务器封装了特定的能力，如文件检索、数据库查询等。
2. MCP客户端（`Client`）：嵌入在`AI`应用中的连接器，与`MCP`服务器建立一对一连接，充当模型与服务器之间的桥梁。它负责发现可用服务、发送调用请求、获取结果，并将这些信息传递给`AI`模型。
3. 宿主应用（`Host`）：运行`LLM`的应用程序或环境，如`Claude`桌面应用、`Cursor IDE`等。宿主通过集成`MCP`客户端，使其内部的模型能够调用外部`MCP`服务器的能力。

`MCP-Client`的工作原理是基于`JSON-RPC 2.0`协议，通过标准化的接口与`MCP`服务器进行通信。它能够自动发现可用的`MCP`服务器及其提供的工具，并将这些信息以结构化的方式提供给大语言模型，使模型能够理解可用的工具及其功能，从而根据用户需求决定何时何地调用这些工具。

## 为什么要自写MCP-Client？

自主开发`MCP-Client`有几个重要的原因和优势：

1. **定制化需求**：市场上的通用`MCP`客户端（例如：`ClaudeDesktop`、`Cursor`、`Cline`等等）可能无法满足特定业务场景的需求。通过自主开发，可以根据企业或个人的具体需求进行定制，比如添加特定的安全验证、数据过滤、或针对特定领域的优化。
2. **系统集成**：将`MCP-Client`与现有系统无缝集成。自主开发的`MCP-Client`可以更好地适配已有的技术栈和架构，减少兼容性问题，提高开发效率。
3. **数据隐私与安全**：对于敏感数据或内部系统，自主开发的`MCP-Client`可以实现更严格的权限控制和数据保护措施，确保敏感信息不会被未授权访问或泄露。
4. **性能优化**：针对特定用例优化性能。例如，对于需要高频率、低延迟访问的场景，可以通过定制`MCP-Client`来减少通信开销，提高响应速度。
5. **扩展功能**：实现标准`MCP`协议之外的增强功能。比如添加高级缓存机制、请求队列管理、负载均衡，或针对特定`AI`模型优化的上下文处理逻辑。
6. **控制和可维护性**：对于依赖`AI`能力的核心业务，自主开发的`MCP-Client`意味着更好的控制能力和可维护性。当需求变化或出现问题时，可以快速进行调整和修复，而不必依赖第三方供应商。
7. **适配多种AI模型**：自主开发的`MCP-Client`可以设计为同时支持多种不同的大语言模型（如`Claude`、`GPT`等），根据任务需求动态选择最适合的模型，提高系统灵活性。
8. **特殊协议支持**：对于需要使用特殊通信协议或数据格式的场景，自主开发可以实现这些非标准需求。
9. **降低依赖风险**：减少对第三方服务的依赖，增强系统的独立性和韧性。如果第三方服务发生变更或中断，自主开发的系统可以更快地适应和调整。
10. **专业知识沉淀**：通过自主开发`MCP-Client`，团队可以积累AI与外部系统集成的专业知识和经验，这对于长期的AI战略和能力建设非常有价值。

实际上，随着AI应用的深入和普及，越来越多的组织开始认识到，AI基础设施（包括`MCP-Client`）是一种战略性资产。自主开发这些组件不仅可以获得更好的技术匹配度，还能在竞争中获得差异化优势，尤其是在AI技术对业务至关重要的领域。

`MCP`协议的开放性恰恰为自主开发`MCP-Client`提供了可能，使得组织和开发者能够在标准化框架下创建适合自己需求的定制解决方案，同时仍然保持与整个生态系统的互操作性。通过自写`MCP-Client`，开发者可以充分利用`AI`大模型的能力，同时保持对系统架构和数据流的完全控制。

## MCP-Client编写

## 工程搭建

在本节实验中，需要大家自己准备一个适配`openai`协议的大模型`API`，例如：`deepseek V3`，`Qwen`系列，`Moonshot`月之暗面等等。

为了编写`MCP Client`，在这里我们直接使用上一节（从0开始实现MCP-Server）中，创建好的工程。

首先创建环境变量`.env`文件，在该文件中我们放入自己的大模型相关信息：

主要包含3个字段：

1. `OPENAI_API_KEY`：大模型的`API KEY`
2. `BASE_URL`：大模型请求地址
3. `MODEL`：模型名称

![image-20250421221754858](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250421221754858.png)

这里对模型的种类不限，我使用的是`moonshot`，大家使用其他大模型均可。

## MCP-Prompt

目前支持或深度集成`MCP`协议的大模型，主要包括：**Claude系列**和**GPT系列**等。

国内的大模型供应商对于`MCP`协议基本上没有做针对性的集成训练，所以在国内使用`MCP`协议，必须编写结构化的`MCP-Prompt`，通过`system prompt`的方式让国内的大模型具备适配`MCP`协议。

为了编写这个提示词，我使用`cloudflare`对大模型进行代理，然后对`Cursor`的`MCP`请求进行截获并将`MCP`提示词相关内容保留，其他无关内容删除，得到以下提示词，当然大家也可以自行对这个提示词进行修改，实现自己的定制化。

在工程中创建文件`MCP_Prompt.txt`，将以下内容放入文件中。

```plain
You are an AI assistant, you can help users solve problems, including but not limited to programming, editing files, browsing websites, etc.

====

TOOL USE

You have access to a set of tools that are executed upon the user's approval. You can use one tool per message, and will receive the result of that tool use in the user's response. You use tools step-by-step to accomplish a given task, with each tool use informed by the result of the previous tool use.

# Tool Use Formatting

Tool use is formatted using XML-style tags. The tool name is enclosed in opening and closing tags, and each parameter is similarly enclosed within its own set of tags. Here's the structure:

<tool_name>
<parameter1_name>value1</parameter1_name>
<parameter2_name>value2</parameter2_name>
...
</tool_name>

For example:

<read_file>
<path>src/main.js</path>
</read_file>

Always adhere to this format for the tool use to ensure proper parsing and execution.

# Tools
## use_mcp_tool
Description: Request to use a tool provided by a connected MCP server. Each MCP server can provide multiple tools with different capabilities. Tools have defined input schemas that specify required and optional parameters.
Parameters:
- server_name: (required) The name of the MCP server providing the tool
- tool_name: (required) The name of the tool to execute
- arguments: (required) A JSON object containing the tool's input parameters, following the tool's input schema
Usage:
<use_mcp_tool>
<server_name>server name here</server_name>
<tool_name>tool name here</tool_name>
<arguments>
{
  "param1": "value1",
  "param2": "value2"
}
</arguments>
</use_mcp_tool>

# Tool Use Examples
## Example 1: Requesting to use an MCP tool

<use_mcp_tool>
<server_name>weather-server</server_name>
<tool_name>get_forecast</tool_name>
<arguments>
{
  "city": "San Francisco",
  "days": 5
}
</arguments>
</use_mcp_tool>

## Example 2: Another example of using an MCP tool (where the server name is a unique identifier such as a URL)

<use_mcp_tool>
<server_name>github.com/modelcontextprotocol/servers/tree/main/src/github</server_name>
<tool_name>create_issue</tool_name>
<arguments>
{
  "owner": "octocat",
  "repo": "hello-world",
  "title": "Found a bug",
  "body": "I'm having a problem with this.",
  "labels": ["bug", "help wanted"],
  "assignees": ["octocat"]
}
</arguments>
</use_mcp_tool>

===

MCP SERVERS

The Model Context Protocol (MCP) enables communication between the system and locally running MCP servers that provide additional tools and resources to extend your capabilities.

# Connected MCP Servers

When a server is connected, you can use the server's tools via the `use_mcp_tool` tool, and access the server's resources via the `access_mcp_resource` tool.
<$MCP_INFO$>

===
 
CAPABILITIES
- You have access to MCP servers that may provide additional tools and resources. Each server may provide different capabilities that you can use to accomplish tasks more effectively.

====

RULES
- MCP operations should be used one at a time, similar to other tool usage. Wait for confirmation of success before proceeding with additional operations.

====

OBJECTIVE

You accomplish a given task iteratively, breaking it down into clear steps and working through them methodically.

1. Analyze the user's task and set clear, achievable goals to accomplish it. Prioritize these goals in a logical order.
2. Work through these goals sequentially, utilizing available tools one at a time as necessary. Each goal should correspond to a distinct step in your problem-solving process. You will be informed on the work completed and what's remaining as you go.
3. Once you've completed the user's task, you must use the attempt_completion tool to present the result of the task to the user. 
4. The user may provide feedback, which you can use to make improvements and try again. But DO NOT continue in pointless back and forth conversations, i.e. don't end your responses with questions or offers for further assistance."
```

在这个提示词中，我设置了一个特殊的标记符`<$MCP_INFO$>`，该标记符用于后期载入`MCP Server`及`MCP Tool`相关工具的描述信息。

## Stdio通信协议

`stdio` 传输方式是最简单的通信方式，通常在本地工具之间进行消息传递时使用。它利用标准输入输出（`stdin/stdout`）作为数据传输通道，适用于本地进程间的交互。

在这里我们首先以上一节（从0开始实现MCP-Server）中构建的`weather MCP Server`为例，编写`MCP Client`向`MCP Server`进行请求。

编写`mcp_client_stdio.py`代码如下所示：

```plain
import asyncio
from typing import Optional
from contextlib import AsyncExitStack
import json

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

from dotenv import load_dotenv
import os, re
from openai import OpenAI
from lxml import etree

load_dotenv()  

class MCPClient:
    def __init__(self):
        
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        self.API_KEY = os.getenv("API_KEY")
        self.BASE_URL = os.getenv("BASE_URL")
        self.MODEL = os.getenv("MODEL")
        
        self.client = OpenAI(api_key=self.API_KEY, base_url=self.BASE_URL)
        
        self.messages = []
        
        with open("./MCP_Prompt.txt", "r", encoding="utf-8") as file:
            self.system_prompt = file.read()

    async def connect_to_stdio_server(self, mcp_name, command: str, args: list[str], env: dict[str, str]={}):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        server_params = StdioServerParameters(
            command=command,
            args=args,
            env=env
        )

        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        self.messages.append(
            {
                "role": "system",
                "content": self.system_prompt
            }
        )
        self.messages.append(
            {
                "role": "user",
                "content": query
            }
        )

        
        response = self.client.chat.completions.create(
            model=self.MODEL,
            max_tokens=1024,
            messages=self.messages,
        )

        
        final_text = []
        content = response.choices[0].message.content
        if '<use_mcp_tool>' not in content:
            final_text.append(content)
        else:
            
            server_name, tool_name, tool_args = self.parse_tool_string(content)

            
            result = await self.session.call_tool(tool_name, tool_args)
            print(f"[Calling tool {tool_name} with args {tool_args}]")
            print("-"*40)
            print("Server:", server_name)
            print("Tool:", tool_name)
            print("Args:", tool_args)
            print("-"*40)
            print("Result:", result.content[0].text)
            print("-"*40)
            self.messages.append({
                "role": "assistant",
                "content": content
            })
            self.messages.append({
                "role": "user",
                "content": f"[Tool {tool_name} \n returned: {result}]"
            })

            response = self.client.chat.completions.create(
                model=self.MODEL,
                max_tokens=1024,
                messages=self.messages
            )
            final_text.append(response.choices[0].message.content)
        return "\n".join(final_text)
    
    def parse_tool_string(self, tool_string: str) -> tuple[str, str, dict]:
        """
        解析大模型工具调用返回的字符串
        """
        tool_string = re.findall("(<use_mcp_tool>.*?</use_mcp_tool>)", tool_string, re.S)[0]
        root = etree.fromstring(tool_string)
        server_name = root.find('server_name').text
        tool_name = root.find('tool_name').text
        try:
            tool_args = json.loads(root.find('arguments').text)
        except json.JSONDecodeError:
            raise ValueError("Invalid tool arguments")
        return server_name, tool_name, tool_args

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")
        self.messages = []
        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break
                if query.strip() == '':
                    print("Please enter a query.")
                    continue
                response = await self.process_query(query)
                print(response)

            except Exception as e:
                print(f"\nError: {str(e)}")

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
        await client.connect_to_stdio_server('weather', 'python', ['weather.py'])
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

注意：这里调用的是上一节中编写的`weather MCP Server`，所以`weather.py`文件应该和`mcp_client_stdio.py`文件在同一目录下。

代码执行效果演示：

![image-20250421223247549](https://ming-log.oss-cn-hangzhou.aliyuncs.com/img/image-20250421223247549.png)

`MCP Server`调用成功！

## SSE通信协议

`SSE` 是基于 `HTTP` 协议的流式传输机制，它允许服务器通过 `HTTP` 单向推送事件到客户端。`SSE` 适用于客户端需要接收服务器推送的场景，通常用于实时数据更新。

在这里同样可以以上一节中，我们在公网中搭建的`weather MCP SSE Server`。

编写代码`mcp_client_sse.py`如下所示：

```plain
import asyncio
from typing import Optional
from contextlib import AsyncExitStack
import json

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

from dotenv import load_dotenv
import os, re
from openai import OpenAI
from lxml import etree

load_dotenv()  

class MCPClient:
    def __init__(self):
        
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        self.API_KEY = os.getenv("API_KEY")
        self.BASE_URL = os.getenv("BASE_URL")
        self.MODEL = os.getenv("MODEL")
        
        self.client = OpenAI(api_key=self.API_KEY, base_url=self.BASE_URL)
        
        self.messages = []
        
        with open("./MCP_Prompt.txt", "r", encoding="utf-8") as file:
            self.system_prompt = file.read()

    async def connect_to_sse_server(self, mcp_name, server_url: str):
        stdio_transport = await self.exit_stack.enter_async_context(sse_client(server_url))
        self.sse, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.sse, self.write))

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>\n")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        self.messages.append(
            {
                "role": "system",
                "content": self.system_prompt
            }
        )
        self.messages.append(
            {
                "role": "user",
                "content": query
            }
        )

        
        response = self.client.chat.completions.create(
            model=self.MODEL,
            max_tokens=1024,
            messages=self.messages,
        )

        
        final_text = []
        content = response.choices[0].message.content
        if '<use_mcp_tool>' not in content:
            final_text.append(content)
        else:
            
            server_name, tool_name, tool_args = self.parse_tool_string(content)

            
            result = await self.session.call_tool(tool_name, tool_args)
            print(f"[Calling tool {tool_name} with args {tool_args}]")
            print("-"*40)
            print("Server:", server_name)
            print("Tool:", tool_name)
            print("Args:", tool_args)
            print("-"*40)
            print("Result:", result.content[0].text)
            print("-"*40)
            self.messages.append({
                "role": "assistant",
                "content": content
            })
            self.messages.append({
                "role": "user",
                "content": f"[Tool {tool_name} \n returned: {result}]"
            })

            response = self.client.chat.completions.create(
                model=self.MODEL,
                max_tokens=1024,
                messages=self.messages
            )
            final_text.append(response.choices[0].message.content)
        return "\n".join(final_text)
    
    def parse_tool_string(self, tool_string: str) -> tuple[str, str, dict]:
        """
        解析大模型工具调用返回的字符串
        """
        tool_string = re.findall("(<use_mcp_tool>.*?</use_mcp_tool>)", tool_string, re.S)[0]
        root = etree.fromstring(tool_string)
        server_name = root.find('server_name').text
        tool_name = root.find('tool_name').text
        try:
            tool_args = json.loads(root.find('arguments').text)
        except json.JSONDecodeError:
            raise ValueError("Invalid tool arguments")
        return server_name, tool_name, tool_args

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")
        self.messages = []
        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break
                if query.strip() == '':
                    print("Please enter a query.")
                    continue
                response = await self.process_query(query)
                print(response)

            except Exception as e:
                print(f"\nError: {str(e)}")

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
         await client.connect_to_sse_server('weather_sse', 'http://47.113.225.16:8000/sse')
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

执行效果演示：

![image-20250421223848425](https://ming-log.oss-cn-hangzhou.aliyuncs.com/img/image-20250421223848425.png)

同样可以成功调用`MCP SSE Server`。

大家可以直接调用我部署好的`MCP SSE Server`，

为了方便大家学习，这里可以直接使用的部署好的服务`http://47.113.225.16:8000/sse`来进行测试。当然也鼓励大家换成其他的服务尝试。

## 采用配置文件进行加载

经过前面的实验，我们现在已经可以通过自己编写的`MCP Client`连接任意`MCP Server`，包括`stdio`和`sse`通信协议，但是都只连接了一个`MCP Server`。用过`Cursor`或其他`MCP Client`应用的同学应该很清楚，他们是通过一个`JSON`配置文件去加载多个`MCP Server`，那么我们自己编写的`MCP Client`能否达到这个效果呢？

当然可以，接下来的实验我们就一起来编写相关代码，实现这个需求。

首先，我们定义自己的`JSON`文件协议，例如：

```plain
{
    "mcpServers": {
      "weather-sse": {
        "isActive": true,
        "type": "stdio",
        "command": "python",
        "args": [
          "weather.py"
        ],
        "name": "weather-sse",
        "env": {}
      },
      "amap-amap-sse": {
        "isActive": true,
        "type": "sse",
        "url": "https://mcp.amap.com/sse?key={高德API KEY}",
        "name": "amap-amap-sse"
      }
    }
  }
```

注意：`{高德API KEY}`大家可以去高德官网注册账号，可以免费获取。

字段意义如下所示：

1. 公共字段：

- `isActive`：用于控制该`MCP Server`是否被激活。
- `type`：`MCP Server`的类型，取值为`stdio`或`sse`
- `name`：`MCP Server`别名。

1. `stdio`相关字段：

- `command`：命令名称。
- `args`：参数列表
- `env`：环境变量字典

1. `sse`相关参数：

- `url`：`SSE MCP Server`服务地址。

编写`mcp_client_mix.py`文件，内容如下所示：

```plain
import asyncio
from typing import Optional
from contextlib import AsyncExitStack
import json

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client

from dotenv import load_dotenv
import os, re
from openai import OpenAI
from lxml import etree

load_dotenv()  

class MCPClient:
    def __init__(self):
        
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        self.API_KEY = os.getenv("API_KEY")
        self.BASE_URL = os.getenv("BASE_URL")
        self.MODEL = os.getenv("MODEL")
        self.client = OpenAI(api_key=self.API_KEY, base_url=self.BASE_URL)
        self.sessions = {}
        self.messages = []
        with open("./MCP_Prompt.txt", "r", encoding="utf-8") as file:
            self.system_prompt = file.read()

    async def mcp_json_config(self, mcp_json_file):
        try:
            with open(mcp_json_file, 'r') as f:
                mcp_config: dict = json.load(f)
        except json.JSONDecodeError:
            raise ValueError("Invalid MCP config")
        servers_config: dict = mcp_config.get('mcpServers', {})
        for k, v in servers_config.items():
            try:
                print('-'*50)
                if v.get('isActive', False) == False:
                    continue
                mcp_name = v.get('name', k)
                mcp_type: str = v.get('type', 'stdio')
                if mcp_type.lower() == 'stdio':
                    command = v.get('command', None)
                    args = v.get('args', [])
                    env = v.get('env', {})
                    if command is None:
                        raise ValueError(f'{mcp_name} command is empty.')
                    if args == []:
                        raise ValueError(f'{mcp_name} args is empty.')
                    await self.connect_to_stdio_server(mcp_name, command, args, env)
                elif mcp_type.lower() == 'sse':
                    server_url = v.get('url', None)
                    if server_url is None:
                        raise ValueError(f'{mcp_name} server_url is empty.')
                    await self.connect_to_sse_server(mcp_name, server_url)
                else:
                    raise ValueError(f'{mcp_name} mcp type must in [stdio, sse].')
            except Exception as e:
                print(f"Error connecting to {mcp_name}: {e}")

    async def connect_to_stdio_server(self, mcp_name, command: str, args: list[str], env: dict[str, str]={}):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        server_params = StdioServerParameters(
            command=command,
            args=args,
            env=env
        )

        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        self.sessions[mcp_name] = self.session

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def connect_to_sse_server(self, mcp_name, server_url: str):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        stdio_transport = await self.exit_stack.enter_async_context(sse_client(server_url))
        self.sse, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.sse, self.write))
        self.sessions[mcp_name] = self.session

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>\n")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        self.messages.append(
            {
                "role": "system",
                "content": self.system_prompt
            }
        )
        self.messages.append(
            {
                "role": "user",
                "content": query
            }
        )

        
        response = self.client.chat.completions.create(
            model=self.MODEL,
            max_tokens=1024,
            messages=self.messages,
        )

        
        final_text = []
        content = response.choices[0].message.content
        if '<use_mcp_tool>' not in content:
            final_text.append(content)
        else:
            
            server_name, tool_name, tool_args = self.parse_tool_string(content)

            
            result = await self.sessions[server_name].call_tool(tool_name, tool_args)
            print(f"[Calling tool {tool_name} with args {tool_args}]")
            print("-"*40)
            print("Server:", server_name)
            print("Tool:", tool_name)
            print("Args:", tool_args)
            print("-"*40)
            print("Result:", result.content[0].text)
            print("-"*40)
            self.messages.append({
                "role": "assistant",
                "content": content
            })
            self.messages.append({
                "role": "user",
                "content": f"[Tool {tool_name} \n returned: {result}]"
            })

            response = self.client.chat.completions.create(
                model=self.MODEL,
                max_tokens=1024,
                messages=self.messages
            )
            final_text.append(response.choices[0].message.content)
        return "\n".join(final_text)
    
    def parse_tool_string(self, tool_string: str) -> tuple[str, str, dict]:
        tool_string = re.findall("(<use_mcp_tool>.*?</use_mcp_tool>)", tool_string, re.S)[0]
        root = etree.fromstring(tool_string)
        server_name = root.find('server_name').text
        tool_name = root.find('tool_name').text
        try:
            tool_args = json.loads(root.find('arguments').text)
        except json.JSONDecodeError:
            raise ValueError("Invalid tool arguments")
        return server_name, tool_name, tool_args

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")
        self.messages = []
        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break
                if query.strip() == '':
                    print("Please enter a query.")
                    continue
                response = await self.process_query(query)
                print(response)

            except Exception as e:
                print(f"\nError: {str(e)}")

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
        mcp_config_file = './mcp.json'
        await client.mcp_json_config(mcp_config_file)
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

演示效果如下所示：

![image-20250421230528033](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250421230528033.png)

![image-20250421230615331](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250421230615331.png)

可以看到，两个工具都可以成功调用。

## 值得进一步优化的小建议

在本节中，我们已经实现了通过`MCP`配置文件，加载所有的`MCP Server`，并且经过验证所有的`MCP Server`工具都可以成功调用。

但是，现在的版本无法完成工具之前进行相互调用，无法通过用户的需求调用多个工具配置完成用户问题的解答，基于此大家可以自行修改现有代码，实现的方式不难，大家可以自己动手实操一下。

# 将FastAPI接口转化为MCP Server

## 背景介绍

在当前大模型蓬勃发展的背景下，专为大模型调用设计的”`API`工具”——`MCP`，已成为研究领域的焦点。在之前的两个教程中，我们从零开始指导大家实现了`MCP Server`和`MCP Client`。通过这个过程，相信大家已经意识到`MCP Server`本质上是为大模型提供的一种格式化`API`接口。

说到这里，很多同学可能会思考：市场上已有海量API资源，但它们大多未被转化为`MCP`格式。如果想调用这些`API`，还需要重新编写一遍，确实比较繁琐。那么，有没有一种工具能够将我们现有的`API`一键转换为`MCP`格式呢？

答案是肯定的。今天我们要向大家介绍的正是这样一个项目——`FastAPI-MCP`。该项目实现了从`FastAPI`到`MCP Server`的无缝转换，让我们之前使用`FastAPI`开发的接口能够轻松转化为`MCP Server`，供大模型直接调用，大大提高了开发效率。

`FastAPI-MCP`项目地址：https://github.com/tadata-org/fastapi_mcp

## 效果演示

## 创建工程

首先初始化工程，使用`Python3.10`环境，并进入工程。

```plain
uv init fastapi2mcp -p 3.10
cd fastapi2mcp
```

![image-20250423100130118](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250423100130118.png)

创建虚拟环境

![image-20250423100204077](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250423100204077.png)

激活虚拟环境

![image-20250423100242653](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250423100242653.png)

添加依赖

查看依赖树

![image-20250424100110607](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100110607.png)

到此工程和依赖搭建完成。

## FastAPI接口构建

首先我们使用以下代码，将我们前面介绍的天气查询，封装为一个`FastAPI`接口：

```
app.py
from fastapi import FastAPI
from pydantic import BaseModel
from utils import format_alert, get_weather_from_cityname, get_weather_from_latitude_longitude
import uvicorn

app = FastAPI()

class CityName(BaseModel):
    cityname: str

class CityLatLon(BaseModel):
    latitude: float
    longitude: float

class Weather(BaseModel):
    cityname: str
    weather: str
    temperature: str
    humidity: str
    wind_speed: str

class WeatherResponse(BaseModel):
    code: int
    weather: Weather | None = None
    msg: str

@app.post("/get_weather/cityname", response_model=WeatherResponse, operation_id="get_weather_cityname")
async def get_weather_cityname(city: CityName):
    """
    通过城市名称（中国城市使用拼音）获取天气信息

    Args:
        cityname: 城市名称（中国城市使用拼音）
    
    Returns:
        code: 0 成功 -1 失败
        weather: 天气信息
        msg: 成功或失败信息
    """
    weather_data = await get_weather_from_cityname(city.cityname)
    if weather_data:
        return {"code": 0, "weather": format_alert(weather_data), "msg": "success"}
    else:
        return {"code": -1, "msg": "Failed to fetch weather data"}

@app.post("/get_weather/latitude_longitude", response_model=WeatherResponse, operation_id="get_weather_latitude_longitude")
async def get_weather_latitude_longitude(citylatlon: CityLatLon):
    """
    通过经纬度获取天气信息

    Args:
        latitude: 纬度
        longitude: 经度

    Returns:
        code: 0 成功 -1 失败
        weather: 天气信息
        msg: 成功或失败信息
    """
    weather_data = await get_weather_from_latitude_longitude(citylatlon.latitude, citylatlon.longitude)
    if weather_data:
        return {"code": 0, "weather": format_alert(weather_data), "msg": "success"}
    else:
        return {"code": -1, "msg": "Failed to fetch weather data"}

if __name__ == "__main__":
    uvicorn.run(app, host='0.0.0.0', port=8001)
```

执行代码

![image-20250424100226517](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100226517.png)

接口成功开启。

使用`Postman`测试接口的连通性

![image-20250424100251374](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100251374.png)

接口可以正常返回。

## 将FastAPI接口转化为MCP Server

接下来，给`FastAPI`接口转化为`MCP Server`，转化的方式很简单，类似于给`FastAPI`打上一个补丁。

在`app.py`同级目录下新建脚本`app2mcp.py`。

```plain
from fastapi_mcp import FastApiMCP
import uvicorn
from app import app

mcp = FastApiMCP(
        fastapi=app,
        name="weather-sse",
        description="通过城市名称（中国城市使用拼音）或经纬度获取天气信息",
        describe_all_responses=True,     
        describe_full_response_schema=True  
    )
mcp.mount()

if __name__ == "__main__":
    uvicorn.run(app, host='0.0.0.0', port=8001)
```

直接通过`import`语句将`app.py`中的`FastAPI app`对象导入过来即可。

执行`app2mcp.py`脚本。

![image-20250424100612367](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100612367.png)

通过简单的补丁，我们就已经将`FastAPI`转化为了`MCP Server`。

## MCP Server调用

### 使用Cursor测试MCP Server

在`Cursor`中的`MCP`配置i文件中写入以下内容。

```plain
{
  "mcpServers": {
    "weather-sse": {
      "url": "http://127.0.0.1:8001/mcp",
      "name": "weather-sse"
    }
  }
}
```

保存后，可以看到`Cursor`已经成功连接`MCP Server`

![image-20250424100857447](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100857447.png)

并且服务器终端也有连接请求。

![image-20250424100916378](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100916378.png)

接下来我们在`Cursor`对话窗口测试`MCP Server`功能

![image-20250424100953677](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424100953677.png)

可以看到，可以成功调用。

其他`MCP Client`应用（例如：`Claude-Desktop`、`Cherry Studio`、`Cline`等等）的导入类似，在此不再演示，各位可自行测试。

### 通过我们自写的MCP Client调用

在我们的配置文件中输入以下内容

```plain
{
  "mcpServers": {
    "weather-sse": {
      "isActive": true,
      "type": "sse",
      "url": "http://127.0.0.1:8001/mcp",
      "name": "weather-sse"
    }
  }
}
```

执行`client_openai_mix.py`

![image-20250424101630567](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424101630567.png)

同样可以调用成功。

如果大家对于这块的代码有不清楚的，请看我前面发的文章《从0开始实现`MCP-Client`》。

同时，原来的`FastAPI`接口也是不影响使用的。

![image-20250424104655834](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424104655834.png)

## 控制接口端点的暴露

默认的情况下，`FastAPI-MCP`会将所有的`FastAPI`接口全部暴露给大模型，大模型全部都可以去调用。但是这样其实是不安全的，有些接口可能涉及到一些法律和隐私问题，不便于公开。那么这个时候，我们可以自己去选择要暴露给`MCP Server`的接口就显得非常重要了。

我们可以通过在定义`FastAPI`接口时指定的，`operation_id`和`tags`进行过滤和筛选。

### 通过operation_id进行筛选

通过以下方式，选择`operation_id`为`get_weather_cityname`的接口进行暴露，这个时候其他接口就都不会暴露。

![image-20250424105111967](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424105111967.png)

还可以通过`exclude_operations`对某些接口进行反选，这个大家自行尝试。

可以通过添加打印的方式，获取暴露的所有接口。

![image-20250424105305540](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424105305540.png)

可以看到此时只有`get_weather_cityname`这个接口放入了`MCP Server`。

### 通过tags进行筛选

`operation_id`一般用于控制单个接口的暴露与否，如果我们先要批量去控制某些接口的暴露与否，我们可以使用`tags`进行控制。

我们给前面`FastAPI`中的`get_weather_cityname`工具添加`tags='mcp'`。

![image-20250424105707199](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424105707199.png)

然后就可以在`FastAPI-MCP`中通过`tags`进行筛选。

![image-20250424105746803](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424105746803.png)

通过这样的方式就可以把所有标记为`tags='mcp'`的接口全部暴露出来，没有标记的接口全部都不会暴露给`MCP Server`。

同样在这里也可以使用`exclude_tags`进行`tags`的反选，大家可以自行尝试。

![image-20250424105922623](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424105922623.png)

可以看到此时，同样只有`get_weather_cityname`这个接口放入了`MCP Server`。

## 值得改进的思考

现在的`FastAPI-MCP`还是非常依赖接口注释的详细程度，本质上还是将接口注释机械的放入到提示词中。这个我们可以通过调试看到。

在图示位置添加断点，然后开启调试。

![image-20250424102753910](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424102753910.png)

在调试控制台中输入以下内容，打印工具描述。

```plain
print(mcp.tools[0].description)
```

![image-20250424102833646](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424102833646.png)

工具描述详情

~~~plain
Get Weather Cityname

通过城市名称（中国城市使用拼音）获取天气信息

Args:
    cityname: 城市名称（中国城市使用拼音）

Returns:
    code: 0 成功 -1 失败
    weather: 天气信息
    msg: 成功或失败信息

### Responses:

**200**: Successful Response (Success Response)
Content-Type: application/json

**Example Response:**
```json
{
  "code": 1,
  "msg": "Msg"
}
```

**Output Schema:**
```json
{
  "properties": {
    "code": {
      "type": "integer",
      "title": "Code"
    },
    "weather": {},
    "msg": {
      "type": "string",
      "title": "Msg"
    }
  },
  "type": "object",
  "required": [
    "code",
    "msg"
  ],
  "title": "WeatherResponse"
}
```
**422**: Validation Error
Content-Type: application/json

**Example Response:**
```json
{
  "detail": [
    {
      "loc": [],
      "msg": "Message",
      "type": "Error Type"
    }
  ]
}
```

**Output Schema:**
```json
{
  "properties": {
    "detail": {
      "items": {
        "properties": {
          "loc": {
            "items": {},
            "type": "array",
            "title": "Location"
          },
          "msg": {
            "type": "string",
            "title": "Message"
          },
          "type": {
            "type": "string",
            "title": "Error Type"
          }
        },
        "type": "object",
        "required": [
          "loc",
          "msg",
          "type"
        ],
        "title": "ValidationError"
      },
      "type": "array",
      "title": "Detail"
    }
  },
  "type": "object",
  "title": "HTTPValidationError"
}
```
~~~

实际上，`FastAPI`对于`pydantic`工具集中度非常高，用户如果通过以下方式去定义变量。

```plain
from pydantic import BaseModel, Field

class CityName(BaseModel):
    cityname: str = Field(..., description="城市名称，如果是中国城市，需要使用拼音。")
```

在定义变量时，用户就给出了详细的字段解释，实际上这里的解释是真正需要写入到提示词中的内容，现在的`FastAPI-MCP`版本没有对这样的数据进行获取。用户如果为了适配，`FastAPI-MCP`就比如在每个接口中重复的把字段解释编写一边，显得不太智能和繁琐。

其次，如果是将接口修改为`MCP Server`有可能还会存在一个问题，我们在编写接口时，基本上都是一个格式化的`JSON`作为输出内容。如果直接把`JSON`返回给到大模型，那么返回结果中的每个字段都必须在接口的注释中详细写清楚，如果没有写清楚，并且`JSON`字段的命名可解释性不好，可能就会导致大模型理解出现问题。

如果`FastAPI-MCP`能够对字段解释进行解读，这个问题实际上也是可以得到解决的，因为在定义`FastAPI`接口时，可以指定`response_model`。

![image-20250424104329754](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250424104329754.png)

# Qwen3 MCP实测

## 大模型原生支持MCP协议是什么意思？

在前面的文章中，我们一起从零开始搭建了适配所有大模型的`MCP Client`，无论我们选择什么大模型，都可以完成`MCP`工具的调用。由于我们实现的方式是通过在系统提示词中对大模型的输出内容进行限制，并强制规范了大模型的输出结果，当大模型要进行工具调用时，我们利用提示词要求大模型按照`MCP`协议输出对应的规范化结果。这样做虽然降低了我们大模型的选择门槛，但是同样会带来一些问题，其中最突出的一个问题就是，一但大模型没有按照`MCP`协议的要求输出规范化的结果，那么就会导致`MCP`工具调用失败，正是由于这种不确定性的存在，导致很多企业不放心真正在生产中去使用`MCP`相关工具，仍然停留在测试环节中。

随着大型语言模型（`LLM`）的进一步发展，大模型对于`MCP`协议的支持度，也成为了消费者在选择大模型的上的一个参考标准。大模型“原生支持`MCP`协议”意味着该模型自身具备内置的、遵循 `MCP`（`Model Context Protocol`，模型上下文协议）规范的能力，无需额外的适配层或中间件，就能直接与各种数据源、工具和服务进行双向、安全、高效的通信。这样的原生支持不仅提升了模型在调用外部接口时的速度和稳定性，也简化了开发者的集成工作，让模型与应用间的交互如同“开箱即用”般流畅。

说得通俗一点，原生支持`MCP`协议的大模型，就是模型在微调阶段，使用了`MCP`协议相关的数据对模型进行过微调，使得大模型原生就知道`MCP`协议的规范，那么在调用`MCP`工具时，大大降低大模型出错的可能性。

## Qwen3简单介绍

![image-20250430154355184](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430154355184.png)

4月29日凌晨阿里推出新一代开源大模型`Qwen3`系列，在代码、数学、通用能力等基准测试中达到顶级模型水平（如`DeepSeek-R1`、`o1`、`Grok-3`等）。

**有效融合推理模式和非推理模式，一个模型同时兼具之前QwQ模型（推理模式，用于数学、代码、逻辑推理等场景）和instruct模型（非推理模式，通用对话等场景）的回复能力。**

- **超多尺寸：两款MOE模型：**Qwen3-235B-A22B（2350多亿总参数、 220多亿激活参）、Qwen3-32B（300亿总参数、30亿激活参数），以及**六个Dense模型**：Qwen3-30B-A3B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B、Qwen3-0.6B。
- **推理能力大幅提升：**在数学、代码和逻辑推理等评测中，显著超过QwQ（推理模式）和Qwen2.5-Plus-Instruct（非推理模式），达到同规模业界SOTA水平。
- **模型人类偏好能力显著增强：**创意写作、角色扮演、多轮对话、指令遵循能力均有明显提升，用户体验预期明显更佳，通用能力显著超过Qwen2.5-Plus-Instruct。
- **Agent能力显著增强：**在上述两种模式下都达到目前业界领先水平，能够实现精准的外部工具调用。

此外，`Qwen3`原生支持`MCP`协议，并具备强大的工具调用能力，结合封装了工具调用模板和工具调用解析器的`Qwen-Agent`框架。

## Qwen3的MCP实测

我们可以使用`Qwen-Agent`对`Qwen3`的工具调用能力进行测试。

使用`uv`工具创建测试工程：

```plain
uv init qwen3-mcp-test -p 3.12
```

![image-20250430154917253](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430154917253.png)

进入工程，然后创建虚拟环境

![image-20250430155013780](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430155013780.png)

安装依赖：

```plain
uv add qwen-agent[code-interpreter,gui,mcp,rag]
```

![image-20250430155116543](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430155116543.png)

接下来执行测试代码：

复制

```plain
from qwen_agent.agents import Assistant
from dotenv import load_dotenv
import os

load_dotenv()

API_KEY = os.getenv("API_KEY")
BASE_URL = os.getenv("BASE_URL")
MODEL = os.getenv("MODEL")


llm_cfg = {
    'model': MODEL,
    'model_server': BASE_URL,
    'api_key': API_KEY,
    'enable_thinking': False  
}

tools = [
    {
        "mcpServers": {
            "time-sse": {
            "url": "https://time.mcp.minglog.cn/sse",
            "name": "time-sse"
            }
        }
    },
  'code_interpreter'  
]

bot = Assistant(llm=llm_cfg, function_list=tools)

messages = [{'role': 'user', 'content': '现在几点了？'}]

for responses in bot.run(messages=messages):
    ...
print(responses)
print("___________________")
```

注意，这里我调用的是自己写的`MCP Server`，大家需要自己在工程中另外创建一个`.env`文件，用来存储自己的密钥。

![image-20250430155302050](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430155302050.png)

执行测试代码：

![image-20250430155532042](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430155532042.png)

开启调试，查看输出结果的细节信息。

![image-20250430161133212](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430161133212.png)

可以看到，`responses`一共返回了`5`条结果，依次将每条结果打印出来。

![image-20250430161110870](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250430161110870.png)

从打印结果可以看出，`qwen-agent`框架在处理时，整体的流程和`Function Call`的处理流程一致。

1. 首先根据用户的提问，判断是否需要调用工具。
2. 如果需要调用工具，则返回`function_call`字段，并携带工具相关参数。如果不需要调用工具，则直接返回。
3. 调用工具执行相关功能，并将工具的返回结果返回。
4. 将用户的提问和工具的返回结果一起丢给大模型。
5. 大模型综合工具的返回结果和用户的提问，给予用户最终回答。

结论，本质上来讲`MCP`是`Function Call`的一种特殊形式，所以在`Agent`框架中的处理方式和`Function Call`的处理方式一致。

# Streamable HTTP —— MCP协议的传输机制革新

随着人工智能（`AI`）技术的迅猛发展，AI助手与应用程序之间的高效通信变得尤为重要。模型上下文协议（`Model Context Protocol`，简称`MCP`）应运而生，旨在为大型语言模型（`LLMs`）与外部数据源和工具之间提供标准化的接口。在`MCP`的众多特性中，`Streamable HTTP`传输机制作为一种现代化的通信方式，正在逐渐取代传统的`Server-Sent Events（SSE）`传输方式，成为`AI`通信的新标准。

![image-20250513181409075](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513181409075.png)

## MCP协议概述

`MCP`是由`Anthropic`推动的一项开放标准，旨在解决LLMs在执行任务时对外部信息的依赖问题。通过定义一套通用的通信规则和数据格式，`MCP`使得`LLMs`能够动态地获取所需的上下文信息，从而增强其功能和使用范围。`MCP`的核心架构包括：

- **MCP客户端**：在`LLM`或其服务基础设施端的实现，负责构建请求并发送给`MCP`服务器。
- **MCP服务器**：在外部系统端的实现，接收来自`MCP`客户端的请求，与实际的数据源或工具进行交互，并将获取的数据按照`MCP`协议规范格式化后返回给客户端。
- **上下文信息交换**：促进`LLMs`与外部系统之间上下文信息的双向交换。

通过`MCP`，开发者可以更轻松地将AI助手与各种应用程序和数据源集成，实现更高效的`AI`通信。

## 传统SSE的局限性

`Server-Sent Events（SSE）`是一种基于`HTTP`的单向通信协议，允许服务器向客户端推送实时更新。然而，`SSE`在以下方面存在局限性：

- **通信方向受限**：`SSE`仅支持服务器向客户端的单向通信，无法满足双向交互的需求。
- **会话管理缺失**：`SSE`缺乏内建的会话管理机制，难以实现复杂的状态维护。
- **连接恢复能力弱**：在网络中断后，`SSE`的连接恢复能力有限，可能导致数据丢失。
- **数据格式支持有限**：`SSE`主要支持`UTF-8`文本，无法处理多种数据格式。

这些局限性使得`SSE`在现代`AI`应用中逐渐显得力不从心。

## Streamable HTTP的创新之处

`Streamable HTTP`是`MCP`框架中推荐的传输机制，旨在通过标准`HTTP`实现高效、双向的数据流通信。其主要特点包括：

- **单一端点通信**：使用单一`HTTP`端点处理所有`MCP`通信，简化了网络架构。
- **多种响应模式**：支持批量`（JSON）`和流式`（SSE）`响应，满足不同的通信需求。
- **内建会话管理**：通过`Mcp-Session-Id`头部进行会话管理，简化了状态维护。
- **连接可恢复性**：支持在网络中断后恢复`SSE`连接，提高通信的稳定性。
- **灵活的身份验证**：支持多种身份验证方式，增强了安全性。
- **跨域资源共享（CORS）配置**：提供灵活的`CORS`配置，便于Web应用的集成。

这些特性使得`Streamable HTTP`在现代`AI`通信中具有显著优势。

## 技术对比：SSE vs. Streamable HTTP

| 特性                   | 传统`SSE`传输           | `Streamable HTTP`协议（`MCP`） |
| ---------------------- | ----------------------- | ------------------------------ |
| 通信方向               | 单向（服务器 → 客户端） | 双向（客户端 ↔ 服务器）        |
| 会话管理               | 无内建机制              | 基于`Mcp-Session-Id`头部       |
| 数据格式支持           | 仅支持UTF-8文本         | 支持多种数据格式               |
| 自动重连               | 支持                    | 支持                           |
| 与现有基础设施的兼容性 | 高                      | 高                             |

从上述对比可以看出，`Streamable HTTP`在通信灵活性、会话管理和数据格式支持等方面均优于传统的`SSE`传输方式。

## 实现与效果演示

## 基于Streamable HTTP的MCP Server

在前面的文章中，我们一起编写过基于`sse`协议的天气查询`MCP Server`并成功部署到线上。

项目地址：https://gitee.com/ming_log/mcp-server

将基于`sse`协议的`MCP Server`修改为`Streamable HTTP`协议，方法非常简单，只需要将`MCP Server`的执行方式修改为`streamable-http`即可。

![image-20250513175712008](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513175712008.png)

注意：需要首先更新一下`mcp[cli]`的版本为`1.8.0`。

完整代码如下：

复制

```plain
from typing import Any
import httpx
from mcp.server.fastmcp import FastMCP


mcp = FastMCP(
    name="weather",
    host="0.0.0.0",
    port=8002,
    description="通过城市名称（拼音）或经纬度获取天气信息",
    sse_path="/mcp"
)


NWS_API_BASE = "https://api.openweathermap.org/data/2.5/weather"
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"


def kelvin_to_celsius(kelvin: float) -> float:
    return kelvin - 273.15

async def get_weather_from_cityname(cityname: str) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "q": cityname,
        "appid": "24ecadbe4bb3d55cb1f06ea48a41ac51"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None
        

async def get_weather_from_latitude_longitude(latitude: float, longitude: float) -> dict[str, Any] | None:
    """向openweathermap发送请求并进行适当的错误处理。"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "application/geo+json"
    }
    params = {
        "lat": latitude,
        "lon": longitude,
        "appid": "24ecadbe4bb3d55cb1f06ea48a41ac51"
    }
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get(NWS_API_BASE, headers=headers, params=params)
            response.raise_for_status()
            return response.json()
        except Exception:
            return None

def format_alert(feature: dict) -> str:
    """将接口返回的天气信息进行格式化文本输出"""
    if feature["cod"] == 404:
        return "参数异常，请确认城市名称是否正确。"
    elif feature["cod"] == 401:
        return "API key 异常，请确认API key是否正确。"
    elif feature["cod"] == 200:
        return f"""
        City: {feature.get('name', 'Unknown')}
        Weather: {feature.get('weather', [{}])[0].get('description', 'Unknown')}
        Temperature: {kelvin_to_celsius(feature.get('main', {}).get('temp', 0)):.2f}°C
        Humidity: {feature.get('main', {}).get('humidity', 0)}%
        Wind Speed: {feature.get('wind', {}).get('speed', 0):.2f} m/s
        """
    else:
        return "未知错误，请稍后再试。"

@mcp.tool()
async def get_weather_from_cityname_tool(city: str) -> str:
    """Get weather information for a city.

    Args:
        city: City name (e.g., "wuhan"). For Chinese cities, please use pinyin
    """
    data = await get_weather_from_cityname(city)
    return format_alert(data)

@mcp.tool()
async def get_weather_from_latitude_longitude_tool(latitude: float, longitude: float) -> str:
    """Get weather information for a location.

    Args:
        latitude: Latitude of the location
        longitude: Longitude of the location
    """
    data = await get_weather_from_latitude_longitude(latitude, longitude)
    return format_alert(data)

if __name__ == "__main__":
    
    
    print("Starting server...")
    mcp.run(transport='streamable-http')
```

开启服务

![image-20250513175829712](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513175829712.png)

接下来测试该服务是否可以正常使用。

由于`Cursor`对于`Streamable HTTP`协议目前还不支持，所以我们使用`Cherry Studio`来进行测试，大家可以自行下载。官网地址：https://www.cherry-ai.com/

下载登陆后，点击左下角的设置。

![image-20250513180108389](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180108389.png)

然后点击`MCP`服务器

![image-20250513180143361](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180143361.png)

接下来点击添加服务器，将我们刚才开启的`MCP Server`配置进去。

![image-20250513180223922](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180223922.png)

按照下图所示的内容填写。

![image-20250513180301435](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180301435.png)

然后点击右上角的保存。

![image-20250513180332988](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180332988.png)

如果没有问题的话，可以看到图示中显示的，服务器更新成功。如果有问题会出现报错。

为了测试`MCP Server`服务，还需要大家准备一个`LLM`，大家根据自己的情况在设置中进行配置即可，我这里选择的是`Moonshot`。

![image-20250513180546680](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180546680.png)

接下来回到聊天助手页面，创建一个聊天助手。在聊天输入框下方选择要使用的`MCP Server`。

![image-20250513180650887](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513180650887.png)

到此，我们的准备工作和配置工作就做完了，接下来我们就可以向聊天助手询问天气，测试`MCP Server`了。

例如：我询问“武汉和北京天气怎么样？”

![PixPin_2025-05-13_17-21-22](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/PixPin_2025-05-13_17-21-22.gif)

`Streamable HTTP`协议在访问`MCP Server`时是并发的，通过以下动图可以看出来，北京的天气是先请求成功的。

![o303p-9167f](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/o303p-9167f.gif)

## 基于Streamable HTTP的MCP Client

在前面的文章中，我们手动编写了`MCP Client`代码，并且同样可以根据`mcp.json`配置文件，加载对应的`MCP Server`服务。当时只适配了`stdio`和`sse`协议，现在加上`Streamable HTTP`协议。

项目地址：https://gitee.com/ming_log/mcp_client

具体代码如下所示：

```plain
import asyncio
from typing import Optional
from contextlib import AsyncExitStack
import json

from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from mcp.client.sse import sse_client
from mcp.client.streamable_http import streamablehttp_client

from dotenv import load_dotenv
import os, re
from openai import OpenAI
from lxml import etree

load_dotenv()  

class MCPClient:
    def __init__(self):
        
        self.session: Optional[ClientSession] = None
        self.exit_stack = AsyncExitStack()
        
        self.API_KEY = os.getenv("API_KEY")
        self.BASE_URL = os.getenv("BASE_URL")
        self.MODEL = os.getenv("MODEL")
        self.client = OpenAI(api_key=self.API_KEY, base_url=self.BASE_URL)
        self.sessions = {}
        self.messages = []
        with open("./MCP_Prompt.txt", "r", encoding="utf-8") as file:
            self.system_prompt = file.read()

    async def mcp_json_config(self, mcp_json_file):
        try:
            with open(mcp_json_file, 'r') as f:
                mcp_config: dict = json.load(f)
        except json.JSONDecodeError:
            raise ValueError("Invalid MCP config")
        servers_config: dict = mcp_config.get('mcpServers', {})
        for k, v in servers_config.items():
            try:
                if v.get('isActive', False) == False:
                    continue
                print('-'*50)
                mcp_name = v.get('name', k)
                mcp_type: str = v.get('type', 'stdio')
                if mcp_type.lower() == 'stdio':
                    command = v.get('command', None)
                    args = v.get('args', [])
                    env = v.get('env', {})
                    if command is None:
                        raise ValueError(f'{mcp_name} command is empty.')
                    if args == []:
                        raise ValueError(f'{mcp_name} args is empty.')
                    await self.connect_to_stdio_server(mcp_name, command, args, env)
                elif mcp_type.lower() == 'sse':
                    server_url = v.get('url', None)
                    if server_url is None:
                        raise ValueError(f'{mcp_name} server_url is empty.')
                    await self.connect_to_sse_server(mcp_name, server_url)
                elif mcp_type.lower() == 'streamable_http':
                    server_url = v.get('url', None)
                    if server_url is None:
                        raise ValueError(f'{mcp_name} server_url is empty.')
                    await self.connect_to_streamable_http_server(mcp_name, server_url)
                else:
                    raise ValueError(f'{mcp_name} mcp type must in [stdio, sse, streamable_http].')
            except Exception as e:
                print(f"Error connecting to {mcp_name}: {e}")

    async def connect_to_stdio_server(self, mcp_name, command: str, args: list[str], env: dict[str, str]={}):
        server_params = StdioServerParameters(
            command=command,
            args=args,
            env=env
        )

        stdio_transport = await self.exit_stack.enter_async_context(stdio_client(server_params))
        self.stdio, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.stdio, self.write))
        self.sessions[mcp_name] = self.session

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def connect_to_sse_server(self, mcp_name, server_url: str):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        stdio_transport = await self.exit_stack.enter_async_context(sse_client(server_url))
        self.sse, self.write = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.sse, self.write))
        self.sessions[mcp_name] = self.session

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>\n")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def connect_to_streamable_http_server(self, mcp_name, server_url: str):
        """Connect to an MCP server

        Args:
            server_script_path: Path to the server script (.py or .js)
        """
        stdio_transport = await self.exit_stack.enter_async_context(streamablehttp_client(server_url))
        self.streamable_http, self.write, _ = stdio_transport
        self.session = await self.exit_stack.enter_async_context(ClientSession(self.streamable_http, self.write))
        self.sessions[mcp_name] = self.session

        await self.session.initialize()
        
        response = await self.session.list_tools()
        available_tools = ['##' + mcp_name + '\n### Available Tools\n- ' + tool.name + "\n" + tool.description + "\n" + json.dumps(tool.inputSchema) for tool in response.tools]
        self.system_prompt = self.system_prompt.replace("<$MCP_INFO$>", "\n".join(available_tools)+"\n<$MCP_INFO$>\n")
        tools = response.tools
        print(f"Successfully connected to {mcp_name} server with tools:", [tool.name for tool in tools])

    async def process_query(self, query: str) -> str:
        """Process a query using Claude and available tools"""
        self.messages.append(
            {
                "role": "system",
                "content": self.system_prompt
            }
        )
        self.messages.append(
            {
                "role": "user",
                "content": query
            }
        )

        
        response = self.client.chat.completions.create(
            model=self.MODEL,
            max_tokens=1024,
            messages=self.messages
        )

        
        final_text = []
        content = response.choices[0].message.content
        if '<use_mcp_tool>' not in content:
            final_text.append(content)
        else:
            
            server_name, tool_name, tool_args = self.parse_tool_string(content)

            
            result = await self.sessions[server_name].call_tool(tool_name, tool_args)
            print(f"[Calling tool {tool_name} with args {tool_args}]")
            print("-"*40)
            print("Server:", server_name)
            print("Tool:", tool_name)
            print("Args:", tool_args)
            print("-"*40)
            print("Result:", result.content[0].text)
            print("-"*40)
            self.messages.append({
                "role": "assistant",
                "content": content
            })
            self.messages.append({
                "role": "user",
                "content": f"[Tool {tool_name} \n returned: {result}]"
            })

            response = self.client.chat.completions.create(
                model=self.MODEL,
                max_tokens=1024,
                messages=self.messages
            )
            final_text.append(response.choices[0].message.content)
        return "\n".join(final_text)
    
    def parse_tool_string(self, tool_string: str) -> tuple[str, str, dict]:
        tool_string = re.findall("(<use_mcp_tool>.*?</use_mcp_tool>)", tool_string, re.S)[0]
        root = etree.fromstring(tool_string)
        server_name = root.find('server_name').text
        tool_name = root.find('tool_name').text
        try:
            tool_args = json.loads(root.find('arguments').text)
        except json.JSONDecodeError:
            raise ValueError("Invalid tool arguments")
        return server_name, tool_name, tool_args

    async def chat_loop(self):
        """Run an interactive chat loop"""
        print("\nMCP Client Started!")
        print("Type your queries or 'quit' to exit.")
        self.messages = []
        while True:
            try:
                query = input("\nQuery: ").strip()

                if query.lower() == 'quit':
                    break
                if query.strip() == '':
                    print("Please enter a query.")
                    continue
                response = await self.process_query(query)
                print(response)

            except Exception as e:
                print(f"\nError: {str(e)}")

    async def cleanup(self):
        """Clean up resources"""
        await self.exit_stack.aclose()

async def main():
    client = MCPClient()
    try:
        
        mcp_config_file = './mcp.json'
        await client.mcp_json_config(mcp_config_file)
        await client.chat_loop()
    finally:
        await client.cleanup()

if __name__ == "__main__":
    asyncio.run(main())
```

接下来，我们使用自己编写的`MCP Client`连接前面的`MCP Server`。

修改`mcp.json`文件内容如下所示：

```plain
{
    "mcpServers": {
      "weather-http": {
        "isActive": true,
        "type": "streamable_http",
        "url": "http://127.0.0.1:8002/mcp",
        "name": "weather-http"
      }
    }
}
```

同样可以成功调用到`MCP Server`的服务。

![image-20250513182033814](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513182033814.png)

多`MCP Server`测试，修改`mcp.json`文件内容如下所示：

```plain
{
    "mcpServers": {
      "time-http": {
        "isActive": true,
        "type": "streamable_http",
        "url": "https://time.mcp.minglog.cn/mcp",
        "name": "time-http"
      },
      "weather-http": {
        "isActive": true,
        "type": "streamable_http",
        "url": "http://127.0.0.1:8002/mcp",
        "name": "weather-http"
      }
    }
}
```

这里我使用了一个我服务器中的`MCP Server`，同样是`streamable_http`协议。

![image-20250513182306857](./7%E6%9C%8816%E6%97%A5%E8%AE%B0%E5%BD%95.assets/image-20250513182306857.png)

所有工具都可以调用成功。

## 结语

`Streamable HTTP`作为`MCP`协议中的推荐传输机制，结合了`HTTP`的广泛兼容性和`SSE`的实时数据推送能力，提供了一种高效、灵活的通信方式。在AI助手与应用程序之间的通信需求日益增长的背景下，`Streamable HTTP`有望成为AI通信的新标准。

如果您希望深入了解`MCP`协议和`Streamable HTTP`的实现细节，可以访问以下资源：

- MCP官方文档：https://modelcontextprotocol.io/
- MCP框架文档：https://mcp-framework.com/docs/Transports/http-stream-transport/
- MCP GitHub仓库：https://github.com/mcp-ai

通过这些资源，您可以更深入地了解`MCP`协议的设计理念和实现方式，探索其在实际应用中的潜力。

